{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SARA3SAEED/aws-ML/blob/main/00_Tutorial_How_To.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FQH45g4EZp0"
      },
      "source": [
        "# Tutorial How-To\n",
        "\n",
        "This tutorial requires this initial notebook to be run first so that the requirements and environment variables are stored for all notebooks in the workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt9nmNAVEZp3"
      },
      "source": [
        "## How to get started\n",
        "\n",
        "1. If you are attending an instructor lead workshop or deployed the workshop infrastructure using the provided [CloudFormation Template](https://raw.githubusercontent.com/aws-samples/prompt-engineering-with-anthropic-claude-v-3/main/cloudformation/workshop-v1-final-cfn.yml) you can proceed to step 2, otherwise you will need to download the workshop [GitHub Repository](https://github.com/aws-samples/prompt-engineering-with-anthropic-claude-v-3) to your local machine.\n",
        "\n",
        "2. Install the required dependencies by running the following command:\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHp4NXJEZp5"
      },
      "source": [
        "> âš ï¸ **Please ignore error messages related to pip's dependency resolver.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5_YQgEezEZp6",
        "outputId": "d3780538-5898-4e8e-c72d-75317b29fe3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[40 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m running egg_info\n",
            "  \u001b[31m   \u001b[0m creating /tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info\n",
            "  \u001b[31m   \u001b[0m writing /tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/PKG-INFO\n",
            "  \u001b[31m   \u001b[0m writing dependency_links to /tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/dependency_links.txt\n",
            "  \u001b[31m   \u001b[0m writing requirements to /tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/requires.txt\n",
            "  \u001b[31m   \u001b[0m writing top-level names to /tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/top_level.txt\n",
            "  \u001b[31m   \u001b[0m writing manifest file '/tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m reading manifest file '/tmp/pip-pip-egg-info-yfqofc69/wikipedia.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
            "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-3voeqwy5/wikipedia_df1ae83e17b7499cb8d329c8735d34ad/setup.py\", line 26, in <module>\n",
            "  \u001b[31m   \u001b[0m     setuptools.setup(\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/__init__.py\", line 108, in setup\n",
            "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/dist.py\", line 956, in run_command\n",
            "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 310, in run\n",
            "  \u001b[31m   \u001b[0m     self.find_sources()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 318, in find_sources\n",
            "  \u001b[31m   \u001b[0m     mm.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 544, in run\n",
            "  \u001b[31m   \u001b[0m     self.prune_file_list()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 610, in prune_file_list\n",
            "  \u001b[31m   \u001b[0m     base_dir = self.distribution.get_fullname()\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 266, in get_fullname\n",
            "  \u001b[31m   \u001b[0m     return _distribution_fullname(self.get_name(), self.get_version())\n",
            "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 284, in _distribution_fullname\n",
            "  \u001b[31m   \u001b[0m     canonicalize_version(version, strip_trailing_zero=False),\n",
            "  \u001b[31m   \u001b[0m TypeError: canonicalize_version() got an unexpected keyword argument 'strip_trailing_zero'\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU pip\n",
        "%pip install -qUr requirements.txt --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VJLZs-REZp8"
      },
      "source": [
        "3. Run the notebook cells in order, following the instructions provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgnutDvEZp8"
      },
      "source": [
        "---\n",
        "\n",
        "## Usage Notes & Tips ðŸ’¡\n",
        "\n",
        "- This course uses Claude 3 Haiku with temperature 0. We will talk more about temperature later in the course. For now, it's enough to understand that these settings yield more deterministic results. All prompt engineering techniques in this course also apply to previous generation legacy Claude models such as Claude 2 and Claude Instant 1.2.\n",
        "\n",
        "- You can use `Shift + Enter` to execute the cell and move to the next one.\n",
        "\n",
        "- When you reach the bottom of a tutorial page, navigate to the next numbered file in the folder, or to the next numbered folder if you're finished with the content within that chapter file.\n",
        "\n",
        "### The Boto3 SDK & the Converse API\n",
        "We will be using the [Amazon Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html) and the [Converse API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) throughout this tutorial.\n",
        "\n",
        "Below is an example of what running a prompt will look like in this tutorial. First, we create `get_completion`, which is a helper function that sends a prompt to Claude and returns Claude's generated response. Run that cell now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y6HISh2EZp9"
      },
      "source": [
        "First, we set and store the model name and region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhjA-WukEZp-"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "from datetime import datetime\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "session = boto3.Session()\n",
        "region = session.region_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WSyeQ6BEZp_",
        "outputId": "449e38b9-9fdd-45c1-bffd-3f548645144a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'modelId' (str)\n",
            "Stored 'region' (str)\n",
            "Using modelId: anthropic.claude-3-haiku-20240307-v1:0\n",
            "Using region:  us-west-2\n"
          ]
        }
      ],
      "source": [
        "#modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
        "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
        "\n",
        "%store modelId\n",
        "%store region\n",
        "\n",
        "print(f'Using modelId: {modelId}')\n",
        "print('Using region: ', region)\n",
        "\n",
        "bedrock_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvP0wPFEZqA"
      },
      "source": [
        "Then, we create `get_completion`, which is a helper function that sends a prompt to Claude and returns Claude's generated response. Run that cell now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "g9Cr-ri2EZqB"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, system_prompt=None):\n",
        "    # Define the inference configuration\n",
        "    inference_config = {\n",
        "        \"temperature\": 0.0,  # Set the temperature for generating diverse responses\n",
        "        \"maxTokens\": 200  # Set the maximum number of tokens to generate\n",
        "    }\n",
        "    # Define additional model fields\n",
        "    additional_model_fields = {\n",
        "        \"top_p\": 1,  # Set the top_p value for nucleus sampling\n",
        "    }\n",
        "    # Create the converse method parameters\n",
        "    converse_api_params = {\n",
        "        \"modelId\": modelId,  # Specify the model ID to use\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],  # Provide the user's prompt\n",
        "        \"inferenceConfig\": inference_config,  # Pass the inference configuration\n",
        "        \"additionalModelRequestFields\": additional_model_fields  # Pass additional model fields\n",
        "    }\n",
        "    # Check if system_text is provided\n",
        "    if system_prompt:\n",
        "        # If system_text is provided, add the system parameter to the converse_params dictionary\n",
        "        converse_api_params[\"system\"] = [{\"text\": system_prompt}]\n",
        "\n",
        "    # Send a request to the Bedrock client to generate a response\n",
        "    try:\n",
        "        response = bedrock_client.converse(**converse_api_params)\n",
        "\n",
        "        # Extract the generated text content from the response\n",
        "        text_content = response['output']['message']['content'][0]['text']\n",
        "\n",
        "        # Return the generated text content\n",
        "        return text_content\n",
        "\n",
        "    except ClientError as err:\n",
        "        message = err.response['Error']['Message']\n",
        "        print(f\"A client error occured: {message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8BrzLDREZqB"
      },
      "source": [
        "Now we will write out an example prompt for Claude and print Claude's output by running our `get_completion` helper function. Running the cell below will print out a response from Claude beneath it.\n",
        "\n",
        "Feel free to play around with the prompt string to elicit different responses from Claude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "O-SnVvzOEZqC",
        "outputId": "467f8035-e130-4864-bac2-eae1bdd4b92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm afraid I don't actually know your favorite color. As an AI assistant, I don't have personal information about you or your preferences. Your favorite color is something unique to you that I don't have access to unless you tell me. What is your favorite color?\n"
          ]
        }
      ],
      "source": [
        "# Prompt\n",
        "prompt = \"what the favort color for me!\"\n",
        "\n",
        "# Get Claude's response\n",
        "print(get_completion(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ApyY2LEZqC"
      },
      "source": [
        "The `modelId` and `region` variables defined earlier will be used throughout the tutorial. Just make sure to run the cells for each tutorial page from top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLQiM5D_EZqD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow2_p310",
      "language": "python",
      "name": "conda_tensorflow2_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}